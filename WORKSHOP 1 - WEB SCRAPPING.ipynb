{
 "cells": [
  {
   "cell_type": "raw",
   "id": "84bfe896",
   "metadata": {},
   "source": [
    "--THIS IS A .IPYNB FILE, OPEN WITH JUPYTER NOTEBOOK\n",
    "--EACH CODE BLOCK IS PRECEEDED BY A PARAGRAPH EXPLAINING IT\n",
    "--FOR QUESTIONS, EMAIL Osama Kheshaifaty at okheshaifaty@athletics.wisc.edu"
   ]
  },
  {
   "cell_type": "raw",
   "id": "9073d2d9",
   "metadata": {},
   "source": [
    "In the provided Python script, the user employs the libraries requests, BeautifulSoup, and pandas to scrape and organize data from a university football roster web page for various years, specifically for the University of Wisconsin Badgers. Initially, a dictionary called universities is created to store the base URL of the university roster page. There's also a list called years ranging from 2010 to 2023 to specify the years of interest. The script iterates through each university (in this case, just one), and within that, iterates through each year in the years list. Depending on the year, it modifies the URL to point to the correct roster page (e.g., appending the year to the base URL, except for 2023 where the base URL is used as-is). There's also commented-out code that shows an alternative way to handle URLs where the year is represented in a different format.\n",
    "\n",
    "Upon constructing the appropriate URL for each year, an HTTP GET request is made using the requests library to fetch the page content, which is then parsed using BeautifulSoup to navigate the HTML structure. The script looks for HTML list items (li tags) with a specific class representing individual players on the roster, although there seems to be a typographical error with the attribute name 'clasdearms' which should likely be corrected to match the actual class name in the HTML. For each player found, it creates a dictionary called player_info to store various pieces of information like name, position, height, weight, academic year, major, hometown, high school, and the specific year of the roster. It extracts this information by finding HTML elements with specific classes and reading their text content. Each player_info dictionary is appended to a list called all_players.\n",
    "\n",
    "Finally, outside the nested loops, the script converts the all_players list of dictionaries to a pandas DataFrame, which is a tabular data structure that's easy to work with and manipulate. This DataFrame is then saved to a CSV file, with the filename derived from the university name and a static string. The to_csv method of pandas DataFrame is used for this purpose, with index=False specified to prevent writing row indices into the CSV file. This way, the script collects, organizes, and saves football roster data for the specified university and years into a structured CSV file, which can be used for further analysis or other purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e4d0cf25",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Requests and Beautifulsoup\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "# List of years for which we want the rosters\n",
    "years = list(range(2010, 2024))\n",
    "\n",
    "# URL structure for each university\n",
    "universities = {\n",
    "    \"UW_Badgers\": \"https://uwbadgers.com/sports/football/roster\",\n",
    "    #add any universities you are interested in here\n",
    "}\n",
    "\n",
    "# Loop through each university\n",
    "for university_name, base_url in universities.items():\n",
    "    # This will be our complete list of players across all years for current university\n",
    "    all_players = []\n",
    "    \n",
    "    # Loop through each year\n",
    "    for year in years:\n",
    "        # Construct the URL for the year\n",
    "        if year == 2023:\n",
    "            url = base_url\n",
    "        else:\n",
    "            url = f\"{base_url}/{year}\"\n",
    "        \n",
    "        #if url has season isntead of year at the end:\n",
    "#        year1 = int(str(year)[-2:])+1\n",
    "#        url = f\"{base_url}{year}-{year1}\"\n",
    "        \n",
    "        response = requests.get(url)\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        \n",
    "        # Find the list item for each player\n",
    "        for player in soup.find_all('li', attrs={'clasdearms': 'si-list-card-item'}):\n",
    "            player_info = {}\n",
    "\n",
    "            # Populate the player data\n",
    "            player_info['Name'] = \" \".join([tag.text for tag in player.find_all('span', {'class':['sidearm-roster-player-first-name', 'sidearm-roster-player-last-name']})])\n",
    "            position = player.find('div', {'class': 'sidearm-list-card-details-item sidearm-roster-player-position-short'})\n",
    "            player_info['Position'] = position.text.strip() if position else None\n",
    "            height = player.find('span', {'class': 'sidearm-roster-player-height'})\n",
    "            player_info['Height'] = height.text.strip() if height else None\n",
    "            weight = player.find('span', {'class': 'sidearm-roster-player-weight'})\n",
    "            player_info['Weight'] = weight.text.strip() if weight else None\n",
    "            academic_year = player.find('span', {'class': 'sidearm-roster-player-academic-year'})\n",
    "            player_info['Academic Year'] = academic_year.text.strip() if academic_year else None\n",
    "            major = player.find('span', {'class': 'sidearm-roster-player-player-major'})\n",
    "            player_info['Major'] = major.text.strip() if major else None\n",
    "            hometown = player.find('span', {'class': 'sidearm-roster-player-hometown'})\n",
    "            player_info['Hometown'] = hometown.text.strip() if hometown else None\n",
    "            highschool = player.find('span', {'class': 'sidearm-roster-player-highschool'})\n",
    "            player_info['Highschool'] = highschool.text.strip() if highschool else None\n",
    "            # Add the year\n",
    "            player_info['Year'] = year\n",
    "\n",
    "            all_players.append(player_info)\n",
    "\n",
    "    # Convert to a DataFrame\n",
    "    df = pd.DataFrame(all_players)\n",
    "\n",
    "    # Save to a CSV file named according to the university name\n",
    "    df.to_csv(f'{university_name}_football_players(workshop).csv', index=False)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "a54a3e7f",
   "metadata": {},
   "source": [
    "The given Python script utilizes the Selenium library to automate web browser interaction for scraping football roster data from the Ohio State Buckeyes' website for various years, ranging from 2010 to 2023. Initially, it imports the necessary libraries and initializes a Chrome browser driver. It defines a list of years and the base URL for the roster pages. An empty pandas DataFrame all_years_players is created to store the collected data. The script then enters a loop, iterating through each year, constructing the URL for that year's roster page, and navigating to the page using the Selenium driver. It has logic to handle a special case for the year 2023 where the URL format differs. Once on the page, it attempts to find and click a button to change the view to a table format, waiting up to 3 seconds for the button to appear. If the button isn't found or clicking it fails, it prints an error message and continues to the next iteration of the loop.\n",
    "\n",
    "Upon successfully changing the view to a table, the script waits again for up to 3 seconds for the table to load. If the table doesn't load in time, it prints an error message and continues to the next iteration. Once the table is loaded, it finds all the table rows representing individual players using a CSS selector. It then enters another loop, iterating through each table row. Within this inner loop, it creates an empty dictionary player_info to store the player's data. It extracts various pieces of information from the table cells (such as name, position, height, etc.) by finding the td elements within the row and reading their text content. It also extracts an index value from the class attribute of the row, if present. Each player_info dictionary is appended to a list all_players. After processing all the rows for the current year, it converts the all_players list to a pandas DataFrame year_df, which is then appended to the all_years_players DataFrame to aggregate the data across all years.\n",
    "\n",
    "After processing all years, the script saves the aggregated data to a CSV file using pandas' to_csv method. It specifies index=False to prevent writing row indices into the CSV. It then waits for a second using time.sleep(1) to be polite to the server by not overloading it with rapid requests, before moving onto the next year in the loop. Finally, after exiting the loop and saving the data, it closes the browser window using the driver.quit() method to clean up resources. This script demonstrates a comprehensive approach to web scraping by handling different URL formats, waiting for elements to load, catching exceptions, and aggregating data across multiple pages into a single CSV file for further analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e4d9192",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Selenium\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "# Initialize Chrome Driver\n",
    "driver = webdriver.Chrome()\n",
    "\n",
    "# List of years\n",
    "years = list(range(2010, 2024))\n",
    "\n",
    "# Base URL\n",
    "base_url = \"https://ohiostatebuckeyes.com/sports/football/roster/\"\n",
    "#replace with the uni you are interested in\n",
    "\n",
    "# Create an empty DataFrame to store all players\n",
    "all_years_players = pd.DataFrame()\n",
    "\n",
    "# Loop through each year\n",
    "for year in years:\n",
    "    # Construct the URL for the year\n",
    "    if year == 2023:\n",
    "        url = \"https://ohiostatebuckeyes.com/sports/football/roster/2023\"\n",
    "        #replace with the university you are interested in\n",
    "    else:\n",
    "        year1 = int(str(year)[-2:])+1\n",
    "        url = f\"{base_url}{year}-{year1}\"\n",
    "        \n",
    "    # Navigate to the URL\n",
    "    driver.get(url)\n",
    "    \n",
    "    # Click the button to change the view to a table\n",
    "    try:\n",
    "        button = WebDriverWait(driver, 3).until(EC.presence_of_element_located((By.ID, \"_viewType_table\")))\n",
    "        button.click()\n",
    "    except:\n",
    "        print(f\"Could not find button for the year {year}\")\n",
    "        continue\n",
    "\n",
    "    # Wait for the table to load\n",
    "    try:\n",
    "        WebDriverWait(driver, 3).until(EC.presence_of_element_located((By.CSS_SELECTOR, '[class^=\"s-table-body__row\"]')))\n",
    "    except:\n",
    "        print(f\"Timed out waiting for page to load for the year {year}\")\n",
    "        continue\n",
    "\n",
    "    # Start scraping\n",
    "    rows = driver.find_elements(by=By.CSS_SELECTOR, value='[class^=\"s-table-body__row\"]')\n",
    "    all_players = []\n",
    "\n",
    "    # Your existing loop to extract player information is here\n",
    "    for row in rows:\n",
    "        player_info = {}\n",
    "        class_attribute = row.get_attribute('class')\n",
    "    \n",
    "        if '--index-' in class_attribute:\n",
    "            index = class_attribute.split('--index-')[1]\n",
    "        else:\n",
    "            index = None\n",
    "\n",
    "        tds = row.find_elements(by=By.TAG_NAME, value='td')\n",
    "    \n",
    "        player_info['Index'] = index\n",
    "        player_info['Name'] = tds[1].text if len(tds) > 1 else None\n",
    "        player_info['Position'] = tds[2].text if len(tds) > 2 else None\n",
    "        player_info['Height'] = tds[3].text if len(tds) > 3 else None\n",
    "        player_info['Weight'] = tds[4].text if len(tds) > 4 else None\n",
    "        player_info['Academic Year'] = tds[5].text if len(tds) > 5 else None\n",
    "        player_info['Hometown / High School'] = tds[6].text if len(tds) > 6 else None\n",
    "        player_info['Previous School'] = tds[7].text if len(tds) > 7 else None\n",
    "        player_info['Year'] = year  # Adding the year\n",
    "    \n",
    "        all_players.append(player_info)    \n",
    "    # Convert to DataFrame and append to all_years_players DataFrame\n",
    "    year_df = pd.DataFrame(all_players)\n",
    "    all_years_players = all_years_players.append(year_df, ignore_index=True)\n",
    "\n",
    "    # Sleep for a few seconds to be polite to the server\n",
    "    time.sleep(1)\n",
    "\n",
    "# Save all_years_players DataFrame to a single CSV\n",
    "all_years_players.to_csv(f'Ohio_football_players.csv', index=False)\n",
    "\n",
    "# Close the browser\n",
    "driver.quit()\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "8b04a2c9",
   "metadata": {},
   "source": [
    "The given Python script is primarily aimed at fetching, processing, and organizing demographic data from the U.S. Census Bureauâ€™s API for the year 2019 using the requests and pandas libraries. Initially, it sets up an API key and constructs a URL for the Census API, specifying the desired data fields, geographic levels, and the API key for authentication. It then makes an HTTP GET request to the API using the requests library, and parses the JSON response into a list of lists, where each inner list represents a row of data, and the first inner list represents the column headers.\n",
    "\n",
    "Upon receiving the data, the script creates a pandas DataFrame from it, using the first row of data as the column names and the remaining rows as the data. It then processes the NAME column to separate city and state information into new columns City and State_Code, using the str.split method to split the NAME column on commas, and str.strip to remove any leading or trailing whitespace from the State_Code values. This step enhances the organization of the dataset by separating the geographical information into more granular columns.\n",
    "\n",
    "Further, it adds new columns to the DataFrame to hold processed versions of some of the fetched data fields. It converts the string values in columns B01002_001E, B01003_001E, and B25010_001E to floating point numbers representing average age, total population, and average household size, respectively. This conversion to a numerical data type facilitates any subsequent numerical computations on these columns.\n",
    "\n",
    "Next, the script calculates the percentage of White and Black populations for each city by dividing the respective population counts (B02001_002E and B02001_003E) by the total population, and multiplying by 100 to convert to percentage. These calculations are stored in new columns Percent_White and Percent_Black.\n",
    "\n",
    "Lastly, it trims down the DataFrame to keep only the relevant columns by specifying a list of column names to keep, effectively discarding the original, unprocessed columns and any other extraneous columns. This step leaves the DataFrame with a clean, organized set of data containing the city name, state code, average age, percentages of White and Black populations, and average household size. The script ends by printing the DataFrame to the console, allowing the user to view the organized and processed data. Through these steps, the script demonstrates a methodical approach to fetching, processing, and organizing external data using Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ce0ef33",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "\n",
    "API_KEY = \"API\"\n",
    "\n",
    "# Fetch data from the Census API\n",
    "url = f\"https://api.census.gov/data/2019/acs/acs5?get=NAME,B01002_001E,B02001_002E,B02001_003E,B25010_001E,B01003_001E&for=place:*&in=state:*&key={API_KEY}\"\n",
    "response = requests.get(url)\n",
    "data = response.json()\n",
    "\n",
    "# Create a DataFrame from the fetched data\n",
    "df = pd.DataFrame(data[1:], columns=data[0])\n",
    "\n",
    "# Separate City and State from the NAME column\n",
    "df['City'], df['State_Code'] = df['NAME'].str.split(',', 1).str\n",
    "df['State_Code'] = df['State_Code'].str.strip()\n",
    "\n",
    "# Additional columns\n",
    "df[\"Average_Age\"] = df[\"B01002_001E\"].astype(float)\n",
    "df[\"Total_Population\"] = df[\"B01003_001E\"].astype(float)\n",
    "df[\"Average_Household_Size\"] = df[\"B25010_001E\"].astype(float)\n",
    "\n",
    "# Calculate Percent White and Percent Black\n",
    "df[\"Percent_White\"] = (df[\"B02001_002E\"].astype(float) / df[\"Total_Population\"]) * 100\n",
    "df[\"Percent_Black\"] = (df[\"B02001_003E\"].astype(float) / df[\"Total_Population\"]) * 100\n",
    "\n",
    "# Keep only the relevant columns\n",
    "df = df[[\"City\", \"State_Code\", \"Average_Age\", \"Percent_White\", \"Percent_Black\", \"Average_Household_Size\"]]\n",
    "\n",
    "print(df)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
